[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. student under supervision of Jennifer Dy, Dana Brooks, and Jan-Willem van de Meent at Northeastern University. My main research interests are machine learning with emphasis on probabilistic programming, deep neural networks, and their applications in biomedical image processing. I am one of the developers of Probabilistic Torch, a library for deep generative models that extends PyTorch. I am also one of the maintainers of the Pytorch distributions module.\n","date":1577854800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1577854800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://alicanb.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a Ph.D. student under supervision of Jennifer Dy, Dana Brooks, and Jan-Willem van de Meent at Northeastern University. My main research interests are machine learning with emphasis on probabilistic programming, deep neural networks, and their applications in biomedical image processing.","tags":null,"title":"","type":"authors"},{"authors":["Kivanc Kose","","Christi Alessi-Fox","Melissa Gill","Caterina Longo","Giovanni Pellacani","Jennifer Dy","Dana H. Brooks","Milind Rajadhyaksha"],"categories":null,"content":"","date":1577854800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577854800,"objectID":"bde27fbd52376a5bdcf4004d25c28c6d","permalink":"http://alicanb.github.io/publication/arxiv-mednet/","publishdate":"2020-01-01T00:00:00-05:00","relpermalink":"/publication/arxiv-mednet/","section":"publication","summary":"In-vivo optical microscopy is advancing into routine clinical practice for non-invasively guiding diagnosis and treatment of cancer and other diseases, and thus beginning to reduce the need for traditional biopsy. However, reading and analysis of the optical microscopic images are generally still qualitative, relying mainly on visual examination. Here we present an automated semantic segmentation method called 'Multiscale Encoder-Decoder Network (MED-Net)' that provides pixel-wise labeling into classes of patterns in a quantitative manner. The novelty in our approach is the modeling of textural patterns at multiple scales. This mimics the procedure for examining pathology images, which routinely starts with low magnification (low resolution, large field of view) followed by closer inspection of suspicious areas with higher magnification (higher resolution, smaller fields of view). We trained and tested our model on non-overlapping partitions of 117 reflectance confocal microscopy (RCM) mosaics of melanocytic lesions, an extensive dataset for this application, collected at four clinics in the US, and two in Italy. With patient-wise cross-validation, we achieved pixel-wise mean sensitivity and specificity of 70±11% and 95±2%, respectively, with 0.71±0.09 Dice coefficient over six classes. In the scenario, we partitioned the data clinic-wise and tested the generalizability of the model over multiple clinics. In this setting, we achieved pixel-wise mean sensitivity and specificity of 74% and 95%, respectively, with 0.75 Dice coefficient. We compared MED-Net against the state-of-the-art semantic segmentation models and achieved better quantitative segmentation performance. Our results also suggest that, due to its nested multiscale architecture, the MED-Net model annotated RCM mosaics more coherently, avoiding unrealistic-fragmented annotations.","tags":["RCM"],"title":"Segmentation of Cellular Patterns in Confocal Images of Melanocytic Lesions in vivo via a Multiscale Encoder-Decoder Network (MED-Net)","type":"publication"},{"authors":["K. Kose","","C. Alessi-Fox","D. H. Brooks","J. G. Dy","M. Rajadhyaksha","M. Gill"],"categories":null,"content":"","date":1576558800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576558800,"objectID":"db65d7b841cad1d3a69506a81e5122ff","permalink":"http://alicanb.github.io/publication/jid-mednet/","publishdate":"2019-12-17T00:00:00-05:00","relpermalink":"/publication/jid-mednet/","section":"publication","summary":"In vivo reflectance confocal microscopy (RCM) enables clinicians to examine lesions’ morphological and cytological information in epidermal and dermal layers, while reducing the need for biopsies. As RCM is being adopted more widely, the workflow is expanding from real-time diagnosis at the bedside to include a “capture, store and forward” model with image interpretation and diagnosis occurring offsite, similar to radiology. As the patient may no longer be present at the time of image interpretation, quality assurance is key during image acquisition. Herein, we introduce a quality assurance process by means of automatically quantifying diagnostically uninformative areas within the lesional area, by using RCM and co-registered dermoscopy images together. We trained and validated a pixel-level segmentation model on 117 RCM mosaics collected by international collaborators. The model delineates diagnostically uninformative areas with 82% sensitivity and 93% specificity. We further tested the model on a separate set of 372 co-registered RCM-dermoscopic image pairs and illustrate how the results of the RCM only model can be improved via a multi-modal (RCM + Dermoscopy) approach, which can help quantify the uninformative regions within the lesional area. Our data suggest that machine learning based automatic quantification offers a feasible objective quality control measure for RCM imaging.","tags":["RCM"],"title":"Utilizing Machine Learning for Image Quality Assessment for Reflectance Confocal Microscopy","type":"publication"},{"authors":["","B. Esmaeili","J. G. Dy","D. H. Brooks","J.W. van de Meent"],"categories":null,"content":"","date":1573448400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573448400,"objectID":"b96759f4cc03fd61d5d0223e5ead00fa","permalink":"http://alicanb.github.io/publication/arxiv-combgen/","publishdate":"2019-11-11T00:00:00-05:00","relpermalink":"/publication/arxiv-combgen/","section":"publication","summary":"We evaluate the ability of variational autoencoders to generalize to unseen examples in domains with a large combinatorial space of feature values. Our experiments systematically evaluate the effect of network width, depth, regularization, and the typical distance between the training and test examples. Increasing network capacity benefits generalization in easy problems, where test-set examples are similar to training examples. In more difficult problems, increasing capacity deteriorates generalization when optimizing the standard VAE objective, but once again improves generalization when we decrease the KL regularization. Our results establish that interplay between model capacity and KL regularization is not clear cut; we need to take the typical distance between train and test examples into account when evaluating generalization.","tags":[],"title":"Evaluating Combinatorial Generalization in Variational Autoencoders","type":"publication"},{"authors":["K. Kose","","J. G. Dy","D. H. Brooks","M. Rajadhyaksha"],"categories":null,"content":"","date":1565236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565236800,"objectID":"74a0179e4e6363c2cee052e866103557","permalink":"http://alicanb.github.io/publication/stanford-fac/","publishdate":"2019-08-08T00:00:00-04:00","relpermalink":"/publication/stanford-fac/","section":"publication","summary":"","tags":["RCM"],"title":"Facilitating the Adoption of Reflectance Confocal Microscopy (RCM) in Clinical Cancer Care Practice with Machine Learning","type":"publication"},{"authors":["B. Esmaeili","H. Wu","S. Jain","","N. Siddarth","B. Paige","J. G. Dy","D. H. Brooks","J.W. van de Meent"],"categories":null,"content":"","date":1554350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554350400,"objectID":"bfcb6303a7aa315dbf67676985622571","permalink":"http://alicanb.github.io/publication/aistats-hfvae/","publishdate":"2019-04-04T00:00:00-04:00","relpermalink":"/publication/aistats-hfvae/","section":"publication","summary":"Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.","tags":[],"title":"Structured Disentagled Representations","type":"publication"},{"authors":["","B. Esmaeili","J. G. Dy","D. H. Brooks","J.W. van de Meent"],"categories":null,"content":"","date":1543899600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543899600,"objectID":"ee9bd90faa18e7c1a08130e4f2627ed9","permalink":"http://alicanb.github.io/publication/cract-vae/","publishdate":"2018-12-04T00:00:00-05:00","relpermalink":"/publication/cract-vae/","section":"publication","summary":"An implicit goal in works on deep generative models is that such models should be able to generate novel examples that were not previously seen in the training data. In this paper, we investigate to what extent this property holds for widely employed variational autoencoder (VAE) architectures. VAEs maximize a lower bound on the log marginal likelihood, which implies that they will in principle overfit the training data when provided with a sufficiently expressive decoder. In the limit of an infinite capacity decoder, the optimal generative model is a uniform mixture over the training data. More generally, an optimal decoder should output a weighted average over the examples in the training data, where the magnitude of the weights is determined by the proximity in the latent space. This leads to the hypothesis that, for a sufficiently high capacity encoder and decoder, the VAE decoder will perform nearest-neighbor matching according to the coordinates in the latent space. To test this hypothesis, we investigate generalization on the MNIST dataset. We consider both generalization to new examples of previously seen classes, and generalization to the classes that were withheld from the training set. In both cases, we find that reconstructions are closely approximated by nearest neighbors for higher-dimensional parameterizations. When generalizing to unseen classes however, lower-dimensional parameterizations offer a clear advantage.","tags":[],"title":"Can VAEs generate novel examples?","type":"publication"},{"authors":["","K. Kose","C. Alessi-Fox","M. Gill","D. H. Brooks","J. G. Dy","M. Rajadhyaksha"],"categories":null,"content":"","date":1534996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534996800,"objectID":"dd309728546f89943c75d9fbffb41cfb","permalink":"http://alicanb.github.io/publication/miccai-munet/","publishdate":"2018-08-23T00:00:00-04:00","relpermalink":"/publication/miccai-munet/","section":"publication","summary":"We describe a new multiresolution 'nested encoder-decoder' convolutional network architecture and use it to annotate morphological patterns in reflectance confocal microscopy (RCM) images of human skin for aiding cancer diagnosis. Skin cancers are the most common types of cancers, melanoma being the deadliest among them. RCM is an effective, non-invasive pre-screening tool for skin cancer diagnosis, with the required cellular resolution. However, images are complex, low-contrast, and highly variable, so that clinicians require months to years of expert-level training to be able to make accurate assessments. In this paper, we address classifying 4 key clinically important structural/textural patterns in RCM images. The occurrence and morphology of these patterns are used by clinicians for diagnosis of melanomas. The large size of RCM images, the large variance of pattern size, the large-scale range over which patterns appear, the class imbalance in collected images, and the lack of fully-labeled images all make this a challenging problem to address, even with automated machine learning tools. We designed a novel nested U-net architecture to cope with these challenges, and a selective loss function to handle partial labeling. Trained and tested on 56 melanoma-suspicious, partially labeled, 12k x 12k pixel images, our network automatically annotated diagnostic patterns with high sensitivity and specificity, providing consistent labels for unlabeled sections of the test images. Providing such annotation will aid clinicians in achieving diagnostic accuracy, and perhaps more important, dramatically facilitate clinical training, thus enabling much more rapid adoption of RCM into widespread clinical use process. In addition, our adaptation of U-net architecture provides an intrinsically multiresolution deep network that may be useful in other challenging biomedical image analysis applications. *First two authors share first authorship.*","tags":["RCM"],"title":"A Multiresolution Convolutional Neural Network with Partial Label Training for Annotating Reflectance Confocal Microscopy Images of Skin","type":"publication"},{"authors":["K. Kose","","C. Alessi-Fox","M. Gill","D. H. Brooks","J. G. Dy","M. Rajadhyaksha"],"categories":null,"content":"","date":1522987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522987200,"objectID":"fc2021bcc1ad6693522278d6c2bebb98","permalink":"http://alicanb.github.io/publication/osa-munet/","publishdate":"2018-04-06T00:00:00-04:00","relpermalink":"/publication/osa-munet/","section":"publication","summary":"Morphological tissue patterns in RCM images are critical in diagnosis of melanocytic lesions. We present a multiresolution deep learning framework that can automatically annotate RCM images for these diagnostic patterns with high sensitivity and specificity.","tags":["RCM"],"title":"A Multiresolution Deep Learning Framework for Automated Annotation of Reflectance Confocal Microscopy Images","type":"publication"},{"authors":["","K. Kose","J. Coll-Font","C. Alessi-Fox","D. H. Brooks","J. G. Dy","M. Rajadhyaksha"],"categories":null,"content":"","date":1500609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500609600,"objectID":"d046a88f21790edc298915aabea5fbd5","permalink":"http://alicanb.github.io/publication/ml4h-dej/","publishdate":"2017-07-21T00:00:00-04:00","relpermalink":"/publication/ml4h-dej/","section":"publication","summary":"Reflectance confocal microscopy (RCM) is an effective, non-invasive pre-screening tool for skin cancer diagnosis, but it requires extensive training and experience to assess accurately. There are few quantitative tools available to standardize image acquisition and analysis, and the ones that are available are not interpretable. In this study, we use a recurrent neural network with attention on convolutional network features. We apply it to delineate skin strata in vertically-oriented stacks of transverse RCM image slices in an interpretable manner. We introduce a new attention mechanism called Toeplitz attention, which constrains the attention map to have a Toeplitz structure. Testing our model on an expert labeled dataset of 504 RCM stacks, we achieve 88.17% image-wise classification accuracy, which is the current state-of-art.","tags":["RCM"],"title":"Delineation of Skin Strata in Reflectance Confocal Microscopy Images using Recurrent Convolutional Networks with Toeplitz Attention","type":"publication"},{"authors":["","T. Gale","K. Kose","C. Alessi-Fox","D. H. Brooks","M. Rajadhyaksha","J. G. Dy"],"categories":null,"content":"","date":1500609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500609600,"objectID":"39568413c55b2c8b899fbca3facfebf5","permalink":"http://alicanb.github.io/publication/cvmi-dej/","publishdate":"2017-07-21T00:00:00-04:00","relpermalink":"/publication/cvmi-dej/","section":"publication","summary":"Reflectance confocal microscopy (RCM) is an effective, non-invasive pre-screening tool for cancer diagnosis. However, acquiring and reading RCM images requires extensive training and experience, and novice clinicians exhibit high variance in diagnostic accuracy. Consequently, there is a compelling need for quantitative tools to standardize image acquisition and analysis. In this study, we use deep recurrent convolutional neural networks to delineate skin strata in stacks of RCM images collected at consecutive depths. To perform diagnostic analysis, clinicians collect RCM images at 4-5 specific layers in the tissue. Our model automates this process by discriminating between RCM images of different layers. Testing our model on an expert labeled dataset of 504 RCM stacks, we achieve 87.97% classification accuracy, and 9-fold reduction in the number of anatomically impossible errors compared to the previous state-of-the-art.","tags":["RCM"],"title":"Delineation of Skin Strata in Reflectance Confocal Microscopy Images With Recurrent Convolutional Networks","type":"publication"},{"authors":["K. Kose","","S. Ariafar","C. Alessi-Fox","M. Gill","J. G. Dy","D. H. Brooks","M. Rajadhyaksha"],"categories":null,"content":"","date":1485579600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485579600,"objectID":"56c8336f464525141faf7e52d0ced160","permalink":"http://alicanb.github.io/publication/spie-mosaic/","publishdate":"2017-01-28T00:00:00-05:00","relpermalink":"/publication/spie-mosaic/","section":"publication","summary":"In this study we present a deep learning based classification algorithm for discriminating morphological patterns that appear in RCM mosaics of melanocytic lesions collected at the dermal epidermal junction (DEJ). These patterns are classified into 6 distinct types in the literature: background, meshwork, ring, clod, mixed, and aspecific. Clinicians typically identify these morphological patterns by examination of their textural appearance at 10X magnification. To mimic this process we divided mosaics into smaller regions, which we call tiles, and classify each tile in a deep learning framework. We used previously acquired DEJ mosaics of lesions deemed clinically suspicious, from 20 different patients, which were then labelled according to those 6 types by 2 expert users. We tried three different approaches for classification, all starting with a publicly available convolutional neural network (CNN) trained on natural image, consisting of a series of convolutional layers followed by a series of fully connected layers: (1) We fine-tuned this network using training data from the dataset. (2) Instead, we added an additional fully connected layer before the output layer network and then re-trained only last two layers, (3) We used only the CNN convolutional layers as a feature extractor, encoded the features using a bag of words model, and trained a support vector machine (SVM) classifier. Sensitivity and specificity were generally comparable across the three methods, and in the same ranges as our previous work using SURF features with SVM .  Approach (3) was less computationally intensive to train but more sensitive to unbalanced representation of the 6 classes in the training data. However we expect CNN performance to improve as we add more training data because both the features and the classifier are learned jointly from the data. *First two authors share first authorship.*","tags":[],"title":"Deep learning based classification of morphological patterns in reflectance confocal microscopy to guide noninvasive diagnosis of melanocytic lesions","type":"publication"},{"authors":["","K. Kose","C. Alessi-Fox","J. G. Dy","D. H. Brooks","M. Rajadhyaksha"],"categories":null,"content":"","date":1470974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470974400,"objectID":"ad545db59e2c90926f90f7e61d8c2f9a","permalink":"http://alicanb.github.io/publication/srt-sc/","publishdate":"2016-08-12T00:00:00-04:00","relpermalink":"/publication/srt-sc/","section":"publication","summary":"Measuring the thickness of the stratum corneum (SC) in vivo is often required in pharmacological, dermatological, and cosmetological studies. Reflectance confocal microscopy (RCM) offers a non-invasive imaging-based approach. However, RCM-based measurements currently rely on purely visual analysis of images, which is time-consuming and suffers from inter-user subjectivity. We developed an unsupervised segmentation algorithm that can automatically delineate the SC layer in stacks of RCM images of human skin. We represent the unique textural appearance of SC layer using complex wavelet transform and distinguish it from deeper granular layers of skin using spectral clustering. Moreover, through localized processing in a matrix of small areas (called ‘tiles’), we obtain lateral variation of SC thickness over the entire field of view. On a set of 15 RCM stacks of normal human skin, our method estimated SC thickness with a mean error of 5.4 ± 5.1 μm compared to the ‘ground truth’ segmentation obtained from a clinical expert. Our algorithm provides a non-invasive RCM imaging-based solution which is automated, rapid, objective, and repeatable.","tags":["RCM"],"title":"Unsupervised delineation of stratum corneum using reflectance confocal microscopy and spectral clustering","type":"publication"},{"authors":["J.P. Campbell","E. Ataer-Cansizoglu","V. Bolon-Canedo","","D. Erdogmus","J. Kalpathy-Cramer","S.N. Patel","J.D. Reynolds","J. Horowitz","K. Hutcheson","others"],"categories":null,"content":"","date":1464753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464753600,"objectID":"c0e8686f5a0c2c28f90b6d3c7b63bda5","permalink":"http://alicanb.github.io/publication/opthalmology-rop/","publishdate":"2016-06-01T00:00:00-04:00","relpermalink":"/publication/opthalmology-rop/","section":"publication","summary":"*Importance: Published definitions of plus disease in retinopathy of prematurity (ROP) reference arterial tortuosity and venous dilation within the posterior pole based on a standard published photograph. One possible explanation for limited interexpert reliability for a diagnosis of plus disease is that experts deviate from the published definitions. *Objective: To identify vascular features used by experts for diagnosis of plus disease through quantitative image analysis. *Design, Setting, and Participants: A computer-based image analysis system (Imaging and Informatics in ROP [i-ROP]) was developed using a set of 77 digital fundus images, and the system was designed to classify images compared with a reference standard diagnosis (RSD). System performance was analyzed as a function of the field of view (circular crops with a radius of 1-6 disc diameters) and vessel subtype (arteries only, veins only, or all vessels). Routine ROP screening was conducted from June 29, 2011, to October 14, 2014, in neonatal intensive care units at 8 academic institutions, with a subset of 73 images independently classified by 11 ROP experts for validation. The RSD was compared with the majority diagnosis of experts. *Main Outcomes and Measures: The primary outcome measure was the percentage of accuracy of the i-ROP system classification of plus disease, with the RSD as a function of the field of view and vessel type. Secondary outcome measures included the accuracy of the 11 experts compared with the RSD. *Results: Accuracy of plus disease diagnosis by the i-ROP computer-based system was highest (95%; 95% CI, 94%-95%) when it incorporated vascular tortuosity from both arteries and veins and with the widest field of view (6–disc diameter radius). Accuracy was 90% or less when using only arterial tortuosity and 85% or less using a 2– to 3–disc diameter view similar to the standard published photograph. Diagnostic accuracy of the i-ROP system (95%) was comparable to that of 11 expert physicians (mean 87%, range 79%-99%). *Conclusions and Relevance: Experts in ROP appear to consider findings from beyond the posterior retina when diagnosing plus disease and consider tortuosity of both arteries and veins, in contrast with published definitions. It is feasible for a computer-based image analysis system to perform comparably with ROP experts, using manually segmented images.","tags":["ROP"],"title":"Expert diagnosis of plus disease in retinopathy of prematurity from computer-based image analysis","type":"publication"},{"authors":["E. Ataer-Cansizoglu","V. Bolon-Canedo","J.P. Campbell","","D. Erdogmus","J. Kalpathy-Cramer","S.N. Patel","K. Jonas","R.V.P. Chan","S. Ostmo","M.F. Chiang","on behalf of the i-ROP Research Consortium"],"categories":null,"content":"","date":1446350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446350400,"objectID":"a7258cf6342c85a0d9d489ff1699ca9a","permalink":"http://alicanb.github.io/publication/tvst-rop/","publishdate":"2015-11-01T00:00:00-04:00","relpermalink":"/publication/tvst-rop/","section":"publication","summary":"*Purpose: We developed and evaluated the performance of a novel computer-based image analysis system for grading plus disease in retinopathy of prematurity (ROP), and identified the image features, shapes, and sizes that best correlate with expert diagnosis. *Methods: A dataset of 77 wide-angle retinal images from infants screened for ROP was collected. A reference standard diagnosis was determined for each image by combining image grading from 3 experts with the clinical diagnosis from ophthalmoscopic examination. Manually segmented images were cropped into a range of shapes and sizes, and a computer algorithm was developed to extract tortuosity and dilation features from arteries and veins. Each feature was fed into our system to identify the set of characteristics that yielded the highest-performing system compared to the reference standard, which we refer to as the “i-ROP” system. *Results: Among the tested crop shapes, sizes, and measured features, point-based measurements of arterial and venous tortuosity (combined), and a large circular cropped image (with radius 6 times the disc diameter), provided the highest diagnostic accuracy. The i-ROP system achieved 95% accuracy for classifying preplus and plus disease compared to the reference standard. This was comparable to the performance of the 3 individual experts (96%, 94%, 92%), and significantly higher than the mean performance of 31 nonexperts (81%). *Conclusions: This comprehensive analysis of computer-based plus disease suggests that it may be feasible to develop a fully-automated system based on wide-angle retinal images that performs comparably to expert graders at three-level plus disease discrimination. Translational Relevance: Computer-based image analysis, using objective and quantitative retinal vascular features, has potential to complement clinical ROP diagnosis by ophthalmologists.","tags":["ROP"],"title":"Computer-Based Image Analysis for Plus Disease Diagnosis in Retinopathy of Prematurity: Performance of the “i-ROP” System and Image Features Associated With Expert Diagnosis","type":"publication"},{"authors":["","P. Duygulu","A.E. Cetin"],"categories":null,"content":"","date":1437624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437624000,"objectID":"69c11c9eabb241abf55f9c4b2e81abe2","permalink":"http://alicanb.github.io/publication/sivp-fonts/","publishdate":"2015-07-23T00:00:00-04:00","relpermalink":"/publication/sivp-fonts/","section":"publication","summary":"Recognizing fonts has become an important task in document analysis, due to the increasing number of available digital documents in different fonts and emphases. A generic font recognition system independent of language, script and content is desirable for processing various types of documents. At the same time, categorizing calligraphy styles in handwritten manuscripts is important for paleographic analysis, but has not been studied sufficiently in the literature. We address the font recognition problem as analysis and categorization of textures. We extract features using complex wavelet transform and use support vector machines for classification. Extensive experimental evaluations on different datasets in four languages and comparisons with state-of-the-art studies show that our proposed method achieves higher recognition accuracy while being computationally simpler. Furthermore, on a new dataset generated from Ottoman manuscripts, we show that the proposed method can also be used for categorizing Ottoman calligraphy with high accuracy.","tags":[],"title":"Classifying fonts and calligraphy styles using complex wavelet transform","type":"publication"},{"authors":["","A. Suhre","A.E. Cetin"],"categories":null,"content":"","date":1407384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1407384000,"objectID":"18f962f94d8bc563ecec330ab6e0d5e6","permalink":"http://alicanb.github.io/publication/sivp-fl/","publishdate":"2014-08-07T00:00:00-04:00","relpermalink":"/publication/sivp-fl/","section":"publication","summary":"Follicular lymphoma (FL) is a group of malignancies of lymphocyte origin that arise from lymph nodes, spleen, and bone marrow in the lymphatic system. It is the second most common non-Hodgkins lymphoma. Characteristic of FL is the presence of follicle center B cells consisting of centrocytes and centroblasts. Typically, FL images are graded by an expert manually counting the centroblasts in an image. This is time consuming. In this paper, we present a novel multi-scale directional filtering scheme and utilize it to classify FL images into different grades. Instead of counting the centroblasts individually, we classify the texture formed by centroblasts. We apply our multi-scale directional filtering scheme in two scales and along eight orientations, and use the mean and the standard deviation of each filter output as feature parameters. For classification, we use support vector machines with the radial basis function kernel. We map the features into two dimensions using linear discriminant analysis prior to classification. Experimental results are presented.","tags":[],"title":"Multi-scale directional-filtering-based method for follicular lymphoma grading","type":"publication"},{"authors":["M. Tofighi","","K. Kose","A.E. Cetin"],"categories":null,"content":"","date":1402545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1402545600,"objectID":"6adc4693e21ecfb8d44bf828c34388fc","permalink":"http://alicanb.github.io/publication/siu-pocs-deconvolution/","publishdate":"2014-06-12T00:00:00-04:00","relpermalink":"/publication/siu-pocs-deconvolution/","section":"publication","summary":"A new deconvolution algorithm based on making orthogonal projections onto the epigraph set of a convex cost function is presented. In this algorithm, the dimension of the minimization problem is lifted by one and sets corresponding to the cost function and observations are defined. If the utilized cost function is convex in R^N, the corresponding epigraph set is also convex in R^(N+1). The deconvolution algorithm starts with an arbitrary initial estimate in R^(N+1). At each iteration cycle of the algorithm, first deconvolution projections are performed onto the hyperplanes representing observations, then an orthogonal projection is performed onto epigraph of the cost function. The method provides globally optimal solutions for total variation, l1, l2, and entropic cost functions.","tags":[],"title":"Deconvolution using projections onto the epigraph set of a convex cost function","type":"publication"},{"authors":["A.E. Cetin","","O. Gunay","Y.H. Habiboglu","K. Kose","I. Onaran","M. Tofighi","R.A. Sevimli"],"categories":null,"content":"","date":1392267600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1392267600,"objectID":"bba254ba1eb9795b9516fa305a3b5dcb","permalink":"http://alicanb.github.io/publication/globalsip-pocs/","publishdate":"2014-02-13T00:00:00-05:00","relpermalink":"/publication/globalsip-pocs/","section":"publication","summary":"A new optimization technique based on the projections onto convex space (POCS) framework for solving convex and some non-convex optimization problems are presented. The dimension of the minimization problem is lifted by one and sets corresponding to the cost function are defined. If the cost function is a convex function in RN the corresponding set which is the epigraph of the cost function is also a convex set in R^N+1. The iterative optimization approach starts with an arbitrary initial estimate in RN+1 and an orthogonal projection is performed onto one of the sets in a sequential manner at each step of the optimization problem. The method provides globally optimal solutions in total-variation, filtered variation, l_1, and entropic cost functions. It is also experimentally observed that cost functions based on l_p; p ","tags":[],"title":"Projections onto convex sets (POCS) based optimization by lifting","type":"publication"}]